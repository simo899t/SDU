{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abd5TfyCrZsR"
   },
   "source": [
    "# AI512: Introduction to Machine Learning\n",
    "## University of Southern Denmark - IMADA\n",
    "### Fall 2024 - Melih Kandemir\n",
    "\n",
    "---\n",
    "# Exercise 04\n",
    "---\n",
    "- Vapnik-Chervonenkis (VC) Dimension\n",
    "- Bias-Variance Decomposition\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H86hCmferZsS"
   },
   "source": [
    "## VC Dimension\n",
    "1. The goal of this question is to calculate the VC dimension of the hypothesis set of axis aligned rectangles for a feature space of two dimension. Follow the steps bellow:\n",
    "    - Find a pattern of inputs that would maximize the combinations the hypothesis set could provide.\n",
    "    - Consider this dataset: `[(0, 1), (1, 0), (2, 1), (1,2), (1,1)]`. For a subset of size 2 from this dataset respectively, generate a list of all possible label combinations for binary classification using `itertools`.\n",
    "\n",
    "    - Assign labels to datapoints and divide points into two groups `(0,1)` based on their current labeling.\n",
    "\n",
    "    - Create a rectangle containing one group labels.\n",
    "    - Calculate the growth function for this subset.\n",
    "    - Check if the hypothesis set can shatter this subset.\n",
    "    - Calculate the VC dimension for this subset.\n",
    "    - Repeat for subset sizes 3, 4, 5.\n",
    "\n",
    "    - For a subset of size 4, plot 16 different combinations of labels, points, rectangle and Check visually if the hypothesis set can shatter this subset.\n",
    "    \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0zB_sGKiAxQB",
    "outputId": "d715a9c1-b9f8-4960-d698-7fbc7a246c3c"
   },
   "outputs": [],
   "source": [
    "# Solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Decomposition\n",
    "\n",
    "We will do the bias-variance decomposition for polynimial curve fitting. Please follow the steps below:\n",
    "\n",
    "- Generate a data set by using the function $f(x)=\\sin(2\\pi x)$ and add Gaussian noise with standard deviation $\\sigma=0.3$ to the data. Fill the `true_function` and `generate_data` functions below.\n",
    "    - `L` = 100 (number of training datasets)\n",
    "    - `n_train` = 25 (number of data points for training)\n",
    "    - `n_test` = 100 (number of data points for testing)\n",
    "    - Generate `x_train`s from uniform distribution between 0 and 1.\n",
    "    - Generate `x_test` with linspace between 0 and 1.\n",
    "- Import `PolynomialModel` from lecture notes `01_Basic_Concepts`. \n",
    "    - `num_polynomial_degrees` = 10\n",
    "    - As regularization parameter, use $\\lambda\\in [0, 0.001, 0.01, 0,1, 1, 10, 100, 1000]$.\n",
    "- For each $\\lambda$:\n",
    "    - For each dataset:\n",
    "        - Fit the model to the data.\n",
    "        - Predict the test data.\n",
    "    - Calculate the mean of the predictions.\n",
    "    - Calculate the $\\text{bias}^2$ and variance of the predictions. (keep it on a list for later use in plotting)\n",
    "        - $\\text{bias}^2$ = $\\frac{1}{n_{test}}\\sum_{i=1}^{n_{test}}(\\bar{y}_i - f(x_i))^2$ where $\\bar{y}_i$ is the mean of the predictions for $x_i$ and $f(x_i)$ is the true function value for $x_i$.\n",
    "        - variance = $\\frac{1}{n_{test}}\\sum_{i=1}^{n_{test}}(\\bar{y}_i - \\bar{\\bar{y}})^2$\n",
    "    - Calculate the mean squared error of the predictions. (keep it on a list for later use in plotting)\n",
    "    - Plot a figure:\n",
    "        - x axis: x_test\n",
    "        - Left panel: Predictions from fitted polynomial models. \n",
    "            - Use first 20 dataset predictions from training datasets.\n",
    "        - Right panel: \n",
    "            - Mean of the predictions blue line.\n",
    "            - True function green line.\n",
    "- Plot a figure:\n",
    "    - x axis: $\\lambda$\n",
    "    - Plot the $\\text{bias}^2$ as a blue line.\n",
    "    - Plot the variance as a red line.\n",
    "    - Plot the mean squared error as a black line.\n",
    "    - Plot the $\\text{bias}^2$ + variance as a magenta line.\n",
    "    - Use log scale for x axis.\n",
    "- **Discussion**:\n",
    "    - As $\\lambda$ increases, to which value do the model predictions tend and why? Relate this to the bias-variance decomposition.\n",
    "\n",
    "**Note**: This exercise is adapted from the book \"Pattern Recognition and Machine Learning\" by Christopher M. Bishop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution here\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def true_function(x):\n",
    "    \"\"\"\n",
    "        Input: x\n",
    "        Output: sin(2*pi*x)\n",
    "    \"\"\"\n",
    "    # FILL THIS IN\n",
    "    \n",
    "    return \n",
    "\n",
    "def generate_data(f, L=100, n_train=25, n_test=100, noise=0.3):\n",
    "    \"\"\"\n",
    "        Input: \n",
    "            f: true function\n",
    "            L: number of training sets\n",
    "            n_train: number of training data points\n",
    "            n_test: number of test data points\n",
    "            noise: standard deviation of noise\n",
    "        Output:\n",
    "            train_data: list of training data [(x_train_1, y_train_1), ..., (x_train_L, y_train_L)]\n",
    "            test_data: test data (x_test, y_test)\n",
    "    \"\"\"\n",
    "    # FILL THIS IN\n",
    "    return \n",
    "\n",
    "train_data, test_data = generate_data(true_function, L=100, n_train=25, n_test=100, noise=0.3)\n",
    "regularization_factors = [0, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1000]\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
