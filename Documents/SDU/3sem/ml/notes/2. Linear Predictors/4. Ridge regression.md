Take a special case of regularized least squares, where $p=2$ (example of solving for an opt. $w$)
$$h_S := \arg \min_w \frac{1}{m} \sum_{i \in [m]} (w^\top x_i - y_i)^2 + \lambda ||w||_2^2$$
We can rewrite the loss of this optimization in vector form as.$$L_S(w) := \frac{1}{m} (Z w-y)^2 + \lambda w^\top w$$
Let us find the optimal weights that minimize the loss by setting its gradient to zero once again: (skipped because I can't be bothered smh)

$$L_S(w) = \frac{1}{m} w^\top Z^\top Zw - \frac{1}{m} 2 w^\top Z^\top y + \lambda w^\top w$$
$$\Rightarrow w_S = \left(\frac{1}{m} Z^\top Z + \lambda I \right )^{-1} Z^\top y.$$
This again is known as *ridge regression*.

**Note:** finding a lambda which serves an optimal penalty, is normally found with *k-fold Cross Validation* 