Assume a dataset
![[Pasted image 20260119193909.png]]
Data obviously has a linear correlation. So we can denote the hypothesis space more specifically
$$\mathcal{H} := \{h: h(x) = w^T x, w \in \mathbb{R}^k\}$$
Where $w=(w_0,w_1)$

Assume dataset $S=\{(x_i, y_i) : i \in [m]\}$, then squared-error loss for $h$ is:
$$L_S(w) := \frac{1}{m} \sum_{i \in [m]} (w^\top x_i - y_i)^2$$
Then $w_S := \arg \min_w L_S(w)$ to find a good vector for the hypothesis $w$
$$\nabla_w L_S(w) = \frac{2}{m} \sum_{i=1}^{m} x_i (x_i^\top w - y_i) \\
= 0$$
$$\Rightarrow \sum_{i=1}^{m} x_i x_i^\top w_S = \sum_{i=1}^{m} x_i y_i$$
$$\Rightarrow w_S = \Bigg( \sum_{i=1}^{m} x_i x_i^\top \Bigg)^{-1} \sum_{i=1}^{m} x_i y_i$$
This is often refered to as least squares regression, and $w$ as the least squares solution.

We can denote $z_i := (x_i, 1)$, where $Z = vec(z_i,dots.v,z_n)$to show that:  
$$w_S := (Z^\top Z)^{-1} Z^T y.$$
![[Pasted image 20260119200002.png]]
(this is an example of the learned hypothesis $h$ with $w$)
