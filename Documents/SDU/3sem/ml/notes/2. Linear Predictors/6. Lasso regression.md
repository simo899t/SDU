Much like ridge regression where
$$h_S := \arg \min_w \frac{1}{m} \sum_{i \in [m]} (w^\top x_i - y_i)^2 + \lambda ||w||_2^2$$
Where the penalty term is $lambda norm(w)_2^2$ 

In Lasso regression, the penalty term is simply $lambda abs(w)$

This way Lasso “selects” features by letting only the most predictive ones survive while others are pushed exactly to zero. This is unlike ridge regression, where terms can only approach 0.

So with Lasso regression
- If a feature is **strongly correlated with the output**, it tends to survive (keep a nonzero coefficient).
- If a feature is **weakly correlated** or **highly redundant compared with other features**, Lasso often sets it to zero.

This however results in a problem since, $$f'(x) =\begin{cases}  1, & \text{if } x > 0 \\[2mm] -1, & \text{if } x < 0 \\[1mm] \text{undefined}, & \text{if } x = 0 \end{cases}$$
Because of this we cannot solve for $w$ using $nabla L_S (h_S) = 0$

However, we can use gradient descent in the direction $-nabla L_S (h_S)$ (the direction opposite of the greatest uphill) to approach an optimal $w$ where $nabla L_S (h_S)$ is as small as possible.
$$w_{t+1} := w_t - \alpha \nabla_w L_S(w) \vert_{w:=w_t}$$

This approach is called **gradient descent**. It is in use with nearly all modern machine learning approaches. The coefficient $\alpha>0$ is called a **learning rate**.

Gradient descent requires repetitive evaluation of the gradient of the loss with respect to the parameters at every iteration: $L_S(w) \vert_{w:=w_t}$. Hence, its implementation on the computer requires an analytical calculation of this gradient. This may be time consuming for complex loss functions. Deep learning libraries such as PyTorch and TensorFlow allow us to automate this process. See an example PyTorch implementation of Lasso regression below.

### From 02_Linear_Predictors.ipynb
```python
import torch as th
import torch.nn as nn
import torch.nn.functional as F
import torch.optim

class LassoRegression(nn.Module):
	def __init__(self, n_dims, lambda_coef=1):
	super(LassoRegression, self).__init__()
	self.lambda_coef = lambda_coef
	self.emp_risk = nn.MSELoss()
	self.weight = nn.Parameter(th.randn((n_dims,1)))
	self.bias = nn.Parameter(th.randn((1)))
	
	def predict(self, input):
		return input @ self.weight + self.bias
	
	def learn(self, inputs, labels, num_steps=1):
		# The in-built Stochastic Gradient Descent optimizer
		# The argument "lr" sets the learning rate
		optimizer = torch.optim.SGD(self.parameters(), lr=0.01)
		
		for ii in range(num_steps):
			# Predict with the current weight values
			# This step is called a "forward pass"
			
			predictions = self.predict(inputs)
			loss = ((predictions - labels)**2).mean() \
					+ self.weight.abs().sum()*self.lambda_coef
			
			# Clear the gradient values remaining from
			# the previous iteration
			optimizer.zero_grad()
			
			# Compute the new gradient values
			# This step is called the "backward pass"
			loss.backward()
			
			# Take the gradient descent step
			optimizer.step()

# Convert data into the Torch format
X_train = torch.tensor(X_train).float()
X_test = torch.tensor(X_test).float()
y_train = torch.tensor(y_train).float().reshape(-1,1)
y_test = torch.tensor(y_test).float().reshape(-1,1)

# z-score normalization
m = th.mean(X_train,axis=0)
std = th.std(X_train,axis=0)
X_train = (X_train-m)/std
X_test = (X_test-m)/std
# Train our model
model_lasso = LassoRegression(n_dims=X_train.shape[1], lambda_coef=1)

# Number of gradient descent iterations
num_iterations = 250

# Collect the train and test errors here.
train_errors = np.zeros(num_iterations)
test_errors = np.zeros(num_iterations)

for ii in range(num_iterations):
	model_lasso.learn(X_train, y_train)
	predictions = model_lasso.predict(X_train)
	train_error = ((predictions - y_train)**2).mean().sqrt()
	train_errors[ii] = train_error.detach().numpy()
	
	# Test our model
	predictions = model_lasso.predict(X_test)
	test_error = ((predictions - y_test)**2).mean().sqrt()
	test_errors[ii] = test_error.detach().numpy()

# Plot the learning curve
plt.plot(np.arange(num_iterations),train_errors,'b-', label="Train RMSE")
plt.plot(np.arange(num_iterations),test_errors,'r-', label="Test RMSE")
plt.xlabel("Iteration")
plt.ylabel("RMSE")
plt.legend(loc="upper right")
plt.show()

```
![[Pasted image 20260119234924.png]]
(plotted graph from lecture)