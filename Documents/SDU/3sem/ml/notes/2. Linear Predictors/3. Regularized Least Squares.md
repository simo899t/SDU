As we saw in [[3. Polynomial curve fitting]], $L_S (h_S) = 0$ by memorizing training data can be achieved but will result in overfitting

Assume a hypothesis space $H$ of finite size $|H|$ and a loss function $L_s (h)$ which fits the data as well as possible and constraining model complexity.
$$h_S := \arg \min_{h \in H} L_S(h)+\lambda |H|$$

With $lambda$ as a **regularization coefficient** and $|H|$ the **regularizer** this is called **Structured Risk Minimization**, one of the biggest achievements of machine learning research in the pre-deep-learning era.

Let us apply this to the least squares problem.
$$h_S := \arg \min_w \frac{1}{m} \sum_{i \in [m]} (w^\top x_i - y_i)^2$$
As we know, when $m$ become very large, the weighs will also become large, and thus result in overfitting. To counter this we force the model to keep weights small with the constraint 
$$h_S := \arg \min_w \frac{1}{m} \sum_{i \in [m]} (w^\top x_i - y_i)^2, \qquad ||w||_p \leq \eta $$

This constrained optimization problem can be expressed equivalently as:

$$h_S := \arg \min_w \max_\lambda \frac{1}{m} \sum_{i \in [m]} (w^\top x_i - y_i)^2 + \lambda (||w||_p^p - \eta)$$
where $lambda >= 0$.

By choosing a large $\lambda$ and dropping the inner $\max$ problem, we can find a reasonable approximation referred to as **regularized least squares**:

$$h_S := \arg \min_w  \frac{1}{m} \sum_{i \in [m]} (w^\top x_i - y_i)^2 + \lambda ||w||_p^p$$
We can solve for $w$ with $nabla L_S (h_S) =0$ to find a good $w$ for the hypothesis