### Formal definitions
“A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured in P, improves with experience E”. [Mitchell, 1997]

So
#### Experience E
Experience $E$ comes from the dataset itself $S={(x_1,y_1),(x_2,y_2),dots,(x_n,y_n)}$.

The goal is to find a hypothesis that accurately represents $S:=\{(x_i, y_i) \overset{i.i.d}{\sim} D\}$
#### Task T
The task is to predict $f(x_1) = y_i$ with $h(x_i) approx y_i$
$$ f(x)approx h(x),forall x in X$$
Task can either be a **Classification** or a **Regression** dependent on whether Y is a continuous set (like $RR$)
#### Performance measure P
Performance is measure via a loss(risk) function $L:Y times Y -> RR^+$ 
Where $$ell(y,y´)$$
where $y$ is the true value of $y$ and $\hat{y}$ is the prediction of $y$

Loss (risk) - measures how bad a single prediction $\hat y$ is compared to the true label $y$.

We define two common choices
- $\ell(y,\hat{y}) := \mathbb{I}(y \neq \hat{y})$, zero-one loss for classification
- $\ell(y,\hat{y}) := (y-\hat{y})^2$, squared error for regression
