#### Restriction
Given $S = \{x_1, \ldots, x_m \} \subset X$, the following set
$$\mathcal{H}_S = \{ (h(x_1), \ldots, h(x_m)) : h \in \mathcal{H} \}$$

is called a **restriction** of $\mathcal{H}$ to $S$. We can do this by
$$|cal(H)|=2^(|S|)$$

In other words, restriction in the discrete-label case (*don't worry about continuous labels here, trust me*), a **restriction** means evaluating hypotheses **only on a finite dataset**. This produces a **finite set of distinct labelings**, even if the original hypothesis space is infinite.

- **Example**
Consider restricting simple dataset with 3 points:$$S={(1,1),(2,1),(1,2)}$$
##### Finding the RestrictionÂ $cal(H)_S$
For each hypothesisÂ $h in cal(H)$, we evaluate it on our 3 points to get a label vectorÂ $mat(h(x_1);h(x_2);h(x_3))$.
**Different linear classifiers might give:**
- Line 1:Â (+1,+1,+1)Â - all points positive
- Line 2:Â (+1,+1,âˆ’1)Â - first two positive, last negative
- Line 3:Â (+1,âˆ’1,+1)Â - first and last positive, middle negative
- Line 4:Â (+1,âˆ’1,âˆ’1)Â - only first positive
- Line 5:Â (âˆ’1,+1,+1)Â - only first negative
- Line 6:Â (âˆ’1,+1,âˆ’1)Â - only middle positive
- Line 7:Â (âˆ’1,âˆ’1,+1)Â - only last positive
- Line 8:Â (âˆ’1,âˆ’1,âˆ’1)Â - all points negative

**Key insight:**Â Even thoughÂ $cal(H)$Â has infinitely many lines, they can only produceÂ **finitely many**Â distinct labelings on our 3-point dataset.



#### Shattering
$\mathcal{H}$ is said to **shatter** $S$ if $|\mathcal{H}_S|= 2^{|S|}$.

In words, a hypothesis class $\mathcal{H}$ shatters from a dataset $S$ if the restriction of $\mathcal{H}$ to $S$ is the set of all functions from $S$ to $\{0,1\}$. ThisÂ means that hypothesis classÂ $cal(H)$Â is so expressive that it can achieveÂ **every possible labeling**Â of the datasetÂ $S$.
use
$$d_(V C)="max"{m:T_(cal(H))(m)=2^m} quad ðŸ˜®$$
**Intuition:**Â If your hypothesis class can shatter a dataset, it means your hypothesis class is "complex enough" to memorize any labeling of those points - even completely random noise!

**Learning Theory Connection:**
- **Good:**Â Expressive enough to capture complex patterns
- **Bad:**Â So expressive it can memorize noise â†’ overfitting risk
- **Key insight:**Â There's a maximum dataset size your hypothesis class can shatter â†’ this is theÂ **VC dimension**

#### Growth function
The growth function, $tau_(cal(H)): NN^+->NN^+$ of $cal(H)$ is defined as
$$tau_(cal(H))(m):=max_(S in X^m)|cal(H)_S|$$
The expression above determines the max number of distinct labels in a model that uses a dataset that is on the domain $X$. 

In this case $cal(m)$ is the number of data points in the dataset (on $X$) that the model uses.
![[assets/image.png]]

When using the growth function to find a shatter-able dataset its to answer the question 
"*Whatâ€™sÂ theÂ largestÂ datasetÂ thatÂ HÂ canÂ fullyÂ control?*"



#### VC dimension
The VC dimension of a hypothesis set $\mathcal{H}$ is the size of the largest dataset that $\mathcal{H}$ can shatter:
$$d_{VC}(\mathcal{H}) = \max \{m: \tau_{\mathcal{H}}(m) = 2^m\}$$
Note that if $d_{VC}(\mathcal{H})=\infty$ then $\mathcal{H}$ is not PAC learnable.



**Theorem 5.4 (The Fundamental Theorem of Statistical Learning).** Assume that $d_{VC}(\mathcal{H}) = d < \infty$. Then, there exist $C_1, C_2 \in \mathbb{R}^+$ such that

* $\mathcal{H}$ has the uniform convergence property with sample complexity
	
	$C_1 \frac{d + \log(1/\delta)}{\epsilon^2} \le m_{\mathcal{H}}^{\text{UC}}(\epsilon, \delta) \le C_2 \frac{d + \log(1/\delta)}{\epsilon^2}$

* $\mathcal{H}$ is agnostic PAC learnable with sample complexity
	
	$C_1 \frac{d + \log(1/\delta)}{\epsilon^2} \le m_{\mathcal{H}}(\epsilon, \delta) \le C_2 \frac{d + \log(1/\delta)}{\epsilon^2}$

* $\mathcal{H}$ is PAC learnable with sample complexity
	
	$C_1 \frac{d + \log(1/\delta)}{\epsilon} \le m_{\mathcal{H}}(\epsilon, \delta) \le C_2 \frac{d \log(1/\epsilon) + \log(1/\delta)}{\epsilon}$


In other words, the following statements are equal:
* $\mathcal{H}$ has the uniform convergence property.
* Any ERM rule is a successful agnostic PAC learner for $\mathcal{H}$.
* $\mathcal{H}$ is agnostic PAC learnable.
* $\mathcal{H}$ is PAC learnable.
* Any ERM rule is a successful PAC learner for $\mathcal{H}$.
* $\mathcal{H}$ has a finite VC-dimension.
