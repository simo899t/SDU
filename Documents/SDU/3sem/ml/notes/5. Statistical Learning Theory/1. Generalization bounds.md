Remember the basic definitions below.

**Definition 5.1 (Generalization error)** Given a hypothesis $h \in \mathcal{H}$, a loss function $\ell: Y \times Y \rightarrow [0,1]$ a data distribution $x,y \sim D$, the generalization error of $h$ is defined as
$$R(h) = P_{x,y \sim D}[\ell(h(x),y)]$$

**Definition 5.2 (Empirical error)** Given a hypothesis $h \in \mathcal{H}$, a loss function $\ell: Y \times Y \rightarrow [0,1]$, and a data set $S = \{(x_1,y_1),...,(x_m,y_m)\}$. The empirical error of $h$ is defined as:$$\widehat{R}_S(h) = \frac{1}{m} \sum_{i=1}^m \ell(h(x_i),y_i)$$
Using these definitions above and the concentration inequalities we covered in, [[7. Inequalities]] we can derive the following theorem.

**Theorem 5.1 (Generalization bound)** Let $\mathcal{H}$ be a finite hypothesis set. Then for any $\delta \in (0,1)$, with probability at least $1-\delta$ over the choice of $S \sim D^m$, for all $h \in \mathcal{H}$

$$R(h) \leq \widehat{R}_S(h) + \sqrt{\frac{\log |\mathcal{H}| + \log \frac{1}{\delta}}{2m}}$$
**Proof** is in Melih's lecture notes (I do not recommend ;))

This inequality is called a generalization bound, because it bounds the generalization error of any hypothesis $h \in \mathcal{H}$ in terms of its empirical error. We can derive a number of interesting consequences from this bound:

- The bound is uniform in the sense that it holds simultaneously for all hypotheses in $\mathcal{H}$.
- The bound is independent of the data distribution $D$ and the loss function $\ell$.
- The bound increases logarithmically with the size of the hypothesis set $|\mathcal{H}|$, that is, a richer hypothesis set is more likely to overfit.
- The bound decreases with the size of the training set $m$, that is, more data is less likely to overfit.
- For a fixed training set size $m$ and two different hypothesis sets $\mathcal{H}_1$ and $\mathcal{H}_2$, the bound prefers the smaller hypothesis set. This is known as **Occam's razor principle**. The principle is introduced by the 14th century theologian William of Ockham. It states that among competing hypotheses, the one with the fewest assumptions should be selected.

This all means that **True error â‰¤ training error + bias** for model complexity. Since training error gets smaller when training, with enough samples $m$, this penalty can be made as small as desired

- The bound gives a **Probably Approximately Correct (PAC)** performance guarantee. The event that any hypothesis in $\mathcal{H}$ is **approximately correct** in the sense that its generalization error is at most $\epsilon$ with probability at least $1-\delta$.