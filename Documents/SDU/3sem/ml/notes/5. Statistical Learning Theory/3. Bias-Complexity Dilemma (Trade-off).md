#### No Free Lunch
Even though a perfect classifier $f$ exists (with $R(f)=0$), for any learning algorithm $A$ and some data distribution $D$.
If you randomly sample a training set $S$ from $D$, there is at least a $1/7$ chance that the algorithm $A$ will output a hypothesis with generalization error at least $1/8$.

No matter how good your algorithm is, there are situations (distributions and training sets) where it will fail to generalize well, even if a perfect solution exists. There is always a non-negligible probability of poor performance.

No algorithm is best at every task

![[8318F6F9-9A11-430C-BA13-35CC06D3F7AA.png]]

The no free lunch theorem tells us only that we need to include a degree of bias to the learner. However, it does not tell anything about its consequences. Inducing too much bias limits the ability of the learner to explain the training observations. Inducing too little bias leads to overfitting. The goal is to find the right balance between the two. This dilemma is known as the **bias-complexity dilemma**. 

#### **Bias–complexity dilemma**
**Bias–complexity dilemma** = the trade-off between **how complex your model is** and **how well it generalizes**.

- **Simple model (low complexity)**
    - Cannot capture all the structure in the data → **high bias** / high approximation error
    - Very stable → **low variance** / low estimation error
    - Risk: underfitting
- **Complex model (high complexity)**
    - Can fit the data very well → **low bias** / low approximation error
    - Sensitive to training noise → **high variance** / high estimation error
    - Risk: overfitting

**Goal:** Pick the complexity where **total generalization error** is minimized.

Let us describe the bias-complexity dilemma in more formal terms. Given an ERM solution $h_S \in \arg \min_{h \in \mathcal{H}} \widehat{R}_S(h)$ for a hypothesis space $\mathcal{H}$, we can decompose its generalization error as below

$$\underbrace{R(h_S)}_{\text{Generalization error}} = \underbrace{\min_{h \in \mathcal{H}} R(h)}_{\text{Approximation error}} + \underbrace{\epsilon_{est}}_{\text{Estimation error}}$$
- **$R(h_S)$**: the true risk (expected error) of the learned hypothesis.
- **$\min_{h \in \mathcal{H}} R(h)$**: The best possible risk within your chosen hypothesis space. This is also called the **approximation error**.
    - This comes from the fact that your hypothesis space $\mathcal{H}$ may not be rich enough to perfectly represent the true function.
    - No matter how much data you have, you can’t beat this.
- **$\epsilon_{\text{est}}$**: the **estimation error**, the extra error due to having only a finite training sample $S$.
	- This decreases as you get more data, because your empirical risk minimizer $h_S$ gets closer to the true best $h^* \in \mathcal{H}$.
	- But if $\mathcal{H}$ is large/complex, $\epsilon_{\text{est}}$ is bigger because you risk overfitting the training data.
	- We can see $\epsilon_{est}$ as a kind of penalty (a **cost of complexity**)

Since for any $\mathcal{H'} \supset \mathcal{H}$, it holds that $$\min_{h' \in \mathcal{H}'} R(h') \leq \min_{h \in \mathcal{H}} R(h)$$We can influence the approximation error by choosing different hypothesis from the hypothesis space. Our task is to traded off approximation and estimation error against each other. 

Increasing the hypothesis space, thereby making our solution more complex will reduce the approximation error, but will increase the estimation error. Since the realizability assumption (Realizability: [[2. PAC Learnability]]) implies a smaller training error, we will observe overfitting in this scenario. The opposite will be true when we restrict the hypothesis space.

#### Bias-Variance decomposition
The bias-complexity dilemma can also be observed from the bias and variance of the estimated values of a regression output. 

Consider a regression problem where observations $(x,y) \sim D$. In this instance we want to estimate the conditional expectation of$$f^*(x)=EE(y|x)$$
With the training set $S = \{(x_1,y_1),...,(x_m,y_m)\}$.

Our **estimator** for $EE[y|x]$ is a hypothesis $h_S \in \mathcal{H}$ that minimizes the mean squared error (MSE). 
The expected squared error of the prediction made by this estimator over the noisy label $y$ and training sample $S$ is given by:

$$E_{S, y|x} \Big [ \Big(y - h_{S}(x) \Big)^2 \Big ] =E_{S, y|x} \Big [ y^2 - 2 y h_{S}(x) + h_{S}(x)^2 \Big ]$$
$$=E_{y|x} [ y^2] + E_{S} [ h_{S}(x)^2 ] - 2 E_{y|x} [ y ] E_{S} [ h_{S}(x) ] $$
$$=E_{y|x} [ y]^2 + Var_{y|x}[y] + E_{S} [ h_{S}(x) ]^2 + Var_{S} [ h_{S}(x) ]- 2 E_{y|x} [ y ]E_{S} [ h_{S}(x) ]$$
$${\text{This is the important part, its not crutial to undertstand everything here}}\over={\underbrace{\Big(E_{y|x} [ y ] -E_{S} [ h_{S}(x) ]\Big)^2}_{\text{Estimator~Bias}}  + \underbrace{Var_{S} [ h_{S}(x) ]}_{\text{Estimator~Variance}}+\underbrace{Var_{y|x}[y]}_{\text{Label~noise~variance}}}$$

Note that $E_{S|x}[h_S(x)] = E_S [h_S(x)]$ and $Var_{S|x}[h_S(x)] = Var_S[h_S(x)]$ since $S$ is collected independently from $x$.

