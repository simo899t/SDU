#### Formal definition
A hypothesis class $\mathcal{H}$ is **nonuniformly learnable** if there exist a function $m_{\mathcal{H}}^{NU}: (0, 1)^2 \times \mathcal{H} \to \mathbb{N}$ and a **learning algorithm** $A$ such that for every $\epsilon, \delta \in (0, 1)$ and every distribution $D$, running $A$ on data set $S=\{(x_i,y_i) \overset{i.i.d.}{\sim} D : i = 1, \ldots, m\}$ with $m \ge m_{\mathcal{H}}^{NU}(\epsilon, \delta, h)$ satisfies $P(\{S \sim D: R(A(S)) \le \min_{h' \in \mathcal{H}} R(h') + \epsilon\}) \geq 1-\delta.$


Unlike uniform learnability (fx Agnostic PAC), where with enough data, the algorithm **can always find a hypothesis close to the best one in $\mathcal{H}$**. This means that uniform learning prepares for the worst hypothesis, even if the data only needs a simple one.

In **Nonuniformly learnability** different hypotheses might need **different amounts of data**.

We **assume** that **for each** hypothesis $h \in \mathcal{H}$, there exists a **sample size $m_{\mathcal{H}}^{NU}(\epsilon, \delta, h)$** that is sufficient to learn it:
$$P(R(A(S)))<=R(h)+epsilon >=1-delta, quad "if" |S|<=m_(cal(H))^("NU")(epsilon, delta, h)$$
Note that an agnostic PAC learnable hypothesis class $\mathcal{H}$ is also nonuniform learnable because $$R(A(S)) \leq \min_{h' \in \mathcal{H}} R(h') + \epsilon$$ results *trivially* in.$$R(A(S)) \leq R(h) + \epsilon, \forall h \in \mathcal{H}$$Hence, the set of nonuniform learnable hypothesis classes is larger than the agnostic PAC learnable hypothesis classes. With this relaxation, our motivation is to increase the model capacity while sustaining its learnability.

If in uniform learning there is a very complex (bad) hypothesis in $cal(h)$, then sample complexity explodes (we need huge amounts of data), so we are forced to restrict $cal(H)$ (Restriction: [[2. PAC Learnability]]) in order to keep learnability.

With this new concept that Each hypothesis $h$ has its **own required sample size**, simple hypotheses can be learned with few samples and the complex hypotheses are still allowed — they just “cost more data”

#### Intuition
With limited data, we restrict ourselves to simple hypotheses, then as data grows, we are allowed to consider more complex hypotheses.