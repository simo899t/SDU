The **expectation** (a.k.a. **expected value**) of a random variable $X$ is defined as:
$$E[X] := \sum_{x \in \Lambda} x P_X(x)$$
Take the three coin tosses example from [[2. Random Variables]] where the random variable $X$ indicates the number of heads, $EE[X] = 1.5$ because $EE[X] = 0 dot 1/8 + 1 dot 3/8 + 2 dot 3/8 + 3 dot 1/8 = 1.5$.

The expectation of a random variable is also called its **mean**. The expectation of a random variable is a measure of its **central tendency**. However, it does not tell us anything about the **spread** of the random variable. 

This can be measured by the **variance** of a random variable which is defined as:
$$"Var"[X] := EE[(X-EE[X])^2] = EE[X^2] - EE[X]^2$$
In the above example, $Var[X] = 0.75$ because $$"Var"[X] = EE[X^2] - EE[X]^2 = 0^2 dot 1/8 + 1^2 dot 3/8 + 2^2 dot 3/8 + 3^2 dot 1/8 - 1.5^2 = 0.75$$
The square-root of the variance is called the **standard deviation** of a random variable. Standard deviation is commonly denoted by $\sigma$. In the above example, $\sigma(x) = \sqrt{0.75} = 0.866$.

#### Moments
Generally speaking, a **moment** of a random variable is defined as:

$EE[(X-EE[X])^k]$

where $k$ is a positive integer. The first moment is the expectation itself. The second moment is the variance. The third moment is called the **skewness** of a random variable. It is a measure of the asymmetry of the distribution of a random variable. The fourth moment is called the **kurtosis** of a random variable. It is a measure of the heaviness of the tails of the distribution of a random variable.

---
## Common rules for Expectance and Variance
$$\mathbb{E}[aX + bY] = a\mathbb{E}[X] + b\mathbb{E}[Y]$$
$$\mathbb{E}[c] = c$$
- Discrete
$$\mathbb{E}[g(X)] = \sum_x g(x) , P(X = x)$$
- Continous
$$\mathbb{E}[g(X)] = \int_{-\infty}^{\infty} g(x) f_X(x) , dx$$
$$\mathbb{E}[aX] = a\mathbb{E}[X]$$
$$\mathbb{E}\left[\sum_{i=1}^n X_i\right] = \sum_{i=1}^n \mathbb{E}[X_i]$$
$$\text{If } X \perp Y: \quad \mathbb{E}[XY] = \mathbb{E}[X] \cdot \mathbb{E}[Y]$$
$$\mathbb{E}[X] = \mathbb{E}[\mathbb{E}[X|Y]]$$
$$\mathrm{Var}[X] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2$$
and so $EE[X^2] = EE[X]^2 + "Var"[X]$
$$\mathrm{Var}(aX) = a^2 \mathrm{Var}(X)$$
$$"Cov"[X,Y] := E[(X-E[X])(Y-E[Y])] = E[X Y] - E[X]E[Y]$$
$$\mathrm{Var}(AB) \neq \mathrm{Var}(A) \cdot \mathrm{Var}(B)$$
above only applies if they are **dependant**
$$\mathrm{Var}(AB) = \mathbb{E}[A]^2 \mathrm{Var}(B) + \mathbb{E}[B]^2 \mathrm{Var}(A) + \mathrm{Var}(A)\mathrm{Var}(B)$$
$$Var[\sum_{i=1}^n X_i] = \sum_{i=1}^n Var[X_i]$$
