**Intuitive meaning of probability:** how often an event happens if you repeat a random experiment many times.
$$P(A) = \lim_{n \rightarrow \infty} \frac{n_A}{n}$$
#### Sample space
The **sample space** is all the possible outcomes.
- Example: Rolling one die → $\Omega = {1, 2, 3, 4, 5, 6}$

#### Event $bold(A)$ 
An **event** is just a subset of the sample space.
- Example: $A = \text{"rolling an even number"} = {2, 4, 6}$

Events are what you actually care about measuring probabilities for.

We say powerset of $Omega$ as $2^(Omega)$, then $sigma(Omega) subset 2^(Omega)$ is an eventspace (collection of subsets of $Omega$). An eventspace must be closed under countable unions, countable intersections, and complements. That means:
1. $\bigcup_{i=1}^{\infty} A_i \in \sigma(\Omega)$ 
2. $\bigcap_{i=1}^{\infty} A_i \in \sigma(\Omega)$
3. $A^c \in \sigma(\Omega)$

For many discrete sample spaces such as the outcome of the roll of a pair of dice, it is possible to assume that $2^\Omega=\sigma(\Omega)$. However, for continuous sample spaces we need to define a $\sigma$-algebra that is smaller than $2^\Omega$. This is because an uncountable set with size $2^(Omega)$ is very unstable and breaks the rules of probability.

Example in a dart game, if we care about the precise hit of a dart arrow, this can become very complex. In dart we resort to only care about the actual points the dart arrow hits. So we make the space smaller.

A **probability measure** is a function $P: \sigma(\Omega) \rightarrow [0,1]$ such that:
1. $P(\Omega) = 1$
2. $P(A) \geq 0$ for all $A \in \sigma(\Omega)$
3. If $A_1, A_2, \dots$ are disjoint events (i.e. $A_i \cap A_j = \emptyset$ for all $i \neq j$\), then $P(\bigcup_{i=1}^{\infty} A_i) = \sum_{i=1}^{\infty} P(A_i)$

We actually want to restrict the events to a **well-behaved σ-algebra**

#### Probability Space: 
A tuple defined as $(\Omega, \sigma(\Omega), P)$.

#### Inclusion-Exclusion Principle:
$$P(A \cup B) = P(A) + P(B) - P(A \cap B)$$
This takes into account the fact that $P(A \cap B)$ is counted twice in $P(A) + P(B)$. A direct consequence of this is that $P(A \cup B) \leq P(A) + P(B)$ which is called the **union bound**.
#### Conditional Probability:
$$P(A|B) = \frac{P(A \cap B)}{P(B)}$$
The intuitive meaning of this is the probability of event $A$ given that event $B$ has occurred. That is, the frequency of event $A$ in the subset of trials where event $B$ has occurred. In mathematical terms
$$P(A|B) = \lim_{n \rightarrow \infty} \frac{n_{A \cap B}}{n_B}$$
The definition of conditional probability can be rewritten as $P(A \cap B) = P(A|B)P(B)$. This is called the *product rule*.

#### **Independence:** 
If the probability of event $A$ is not affected by the occurrence of event $B$, these two events are said to be independent.

In terms of conditional probabilities we can describe this situation as $P(A|B) = P(A)$. Applied to the definition of conditional probability, this means that $A$ and $B$ are independent if and only if $$P(A sect B) = P(A)P(B) quad "independent"$$$$P(A sect B) != P(A)P(B) quad "dependent"$$
#### **Law of Total Probability:** 
If $B_1, B_2, \dots, B_n$ is a partition of $\Omega$, i.e. $B_i \cap B_j = \emptyset$ for all $i \neq j$ and $\bigcup_{i=1}^n B_i = \Omega$, then $$P(A) = \sum_{i=1}^n P(A|B_i)P(B_i)$$
#### **Bayes' Rule:** 
$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$This can be derived from the definition of conditional probability as follows:
$$P(A \cap B) = P(A|B)P(B) = P(B|A)P(A) \Rightarrow P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$
Assume law of total probability $\mathcal{H}$, i.e. $H_i \cap H_j = \emptyset$ for all $i \neq j$ and $\bigcup_{i=1}^n H_i = \mathcal{H}$. Also assume that $D$ is the observed data. Then, Bayes' rule can be written as:
$$P(H_i|D) = \frac{P(D|H_i)P(H_i)}{P(D)} = \frac{P(D|H_i)P(H_i)}{\sum_{j=1}^n P(D|H_j)P(H_j)}$$
Here,
* $P(H_i)$ is our **prior belief** in a hypothesis $H_i$,
* $P(H_i|D)$ is our **posterior belief** in $H_i$ after observing the data $D$,
* $P(D|H_i)$ is the **likelihood** of $H_i$, and $P(D)$ is the **evidence**.
  
The concepts above give a general framework for *statistical inference*: we start with a prior belief, collect new observations, and update our belief based on them. This approach has connections to how humans make decisions under uncertainty.