Markov's inequality
$$P(X >= epsilon) <= (EE[X])/epsilon$$
Therefore $$P(X<epsilon)> 1-EE[X]/epsilon$$
- **Key Properties**
		- **Minimal Requirements**: Only needs $X \geq 0$ - the most general assumption possible.
		- **One-Sided Bound**: Only controls upper tail $P(X \geq \epsilon)$, not deviations around the mean.
		- **No Sample Size Improvement**: The bound doesn't get better with more data.
		- **Often Vacuous**: Can give meaningless bounds like $P \leq 5/3 > 1$.
		
	- **Why It Matters:**
		- **Starting Point**: Foundation for deriving other inequalities
		- **Worst-Case Analysis**: When you know almost nothing about your random variable
		- **Quick Feasibility Check**: "Is this probability even theoretically possible?"
		
	- **Intuition**: If you only know the average, you can't say much about extremes. Like knowing the average income in a city tells you little about how many millionaires live there.


- Chebyshev's inequality
	- With Chebyshev's inequality, we are able to describe the same problem as an expression dependent on n (sample size). 
	- With this, probability of correctness will maybe increase as sample size increases
		- In other words, **the probability that our estimate is “correct” (i.e., close to the true mean) increases as the sample size grows**.
		- This is because of $$P(|X - \mu| \geq \epsilon) \leq \frac{\sigma^2}{\epsilon^2}$$by this as $n->infinity$, $r h s->0$  
	- By this to get probability $<= delta$, you need ​$n<=sigma^2/(n epsilon^2)$ samples

		**Why This Is Better:**
		- **Variance Matters**: If you know values are tightly clustered (small $\sigma^2$), deviations are less likely
		- **Sample Size Scaling**: More data → better estimates (empirical risk gets closer to true risk)
		- **Practical Bounds**: Often gives more meaningful bounds where Markov fails

		**Intuition**: Knowing both the average AND the spread lets you make much better predictions. Like knowing both average height and that heights cluster tightly around the mean.
	- ![[chebyshevs_inequality.png]]
	- 
- WLLN (weak law of large numbers)
	- For i.i.d. samples where, mean $mu$ and $"Var"(X)=sigma^2$ 
	- Then sample mean ($\hat{y}_n$) converges to $mu$ as sample size increases     
	- ![[WLLN.png]]
- Hoeffding's inequality
	- Hoeffding's for intervals not just bounded by [0,1]
	- $$P\left(\frac{1}{n} \sum_{i=1}^n X_i - E[\frac{1}{n} \sum_{i=1}^n X_i] \geq \epsilon\right) \leq e^{-2n\epsilon^2/\sum_{i=1}^n(b_i-a_i)^2}$$
	- ![[cheb_vs_hoeff.png]]
	To get probability $<=delta$, you need $n>=ln(1/delta)/(2epsilon^2)$​ samples
	
	The exponential concentration means that as you collect more training data, your confidence that the empirical risk approximates the true risk grows **exponentially fast**, not just linearly. This is why we can make strong theoretical guarantees about generalization in machine learning!
	

![[which_inequality.png]]