Now consider the case where $\Lambda = \mathbb{R}$. This means a random variable $X$ maps outcomes $ω∈Ω$ to numbers in $RR$.


In this case, we have a **continuous random variable** with the induced probability measure:
$$P_X(A) = P(\{\omega \in \Omega: X(\omega) \in A\})$$

where $A \subset \Lambda =\mathbb{R}$. 

This probability measure has the pre-image of $A$ under $X$ on its argument, i.e. $$P_X(A) = P(X^(-1)(A))$$Where $X^(-1)(A)$ is the pre-image, aka all the inputs that, when put through the function $X$, give a numbers in $A$ (the ones we care about).

A common situation for using $X: \mathbb{R} \rightarrow \mathbb{R}$ is where events are intervals $(a,b)$ on the range of $X$. In this case, we can define a probability measure on $\mathbb{R}$ as follows:
$$P_X(a < x < b) := P(\{\omega \in \Omega: X(\omega) \in (a,b)\})$$

where $a,b \in \mathbb{R}$ and $a < b$. 

### Example
- Bus arrives uniformly at any time in $[0,60]$ minutes
- $X(w)=w$ (random variable is just the arrival time)
- $A=[0,15]$ minutes (we want to know the prop. that the bus arrives in between 15 and 45 minutes.)
- Pre-image is then $X^(-1) (A) =[15,45]$ in $Omega$ 
- Probability is therefore: 
$$P_X (A) = P(X^(-1) (A)) =P(w in [15,45]) = (45-15)/60 = 1/2 $$
The $\sigma$-algebra of $\mathbb{R}$ by the intervals of the form $(a,b)$ is called the **Borel $\sigma$-algebra**. 
### Cumulative distribution function (CDF)
We can also determine probability given that $X$ is $<=$ some number $x$:
$$F_X(x) := P(X \leq x) \quad \forall x \in \mathbb{R}$$
Note that this is a non-decreasing function (probabilities goes up as $x$ increases).

### Probability density function (PDF)

Often, we are interested in interpreting the behavior of a continuous distribution by investigating how densely probability is packed at each point $x$. This is the derivative of $F_X (x)$.
$$f_X(x) := \frac{d}{dx} F_X(x) \quad \forall x \in \mathbb{R}$$
Then, by the fundamental theorem of calculus, we have that :
$$F_X(a) = \int_{-\infty}^a f_X(x) dx$$
Hence an event can be described in terms of the probability density function as follows:
$$P(a < X < b) = F_X(b) - F_X(a) = \int_{-\infty}^b f_X(x) dx - \int_{-\infty}^a f_X(x) dx = \int_{a}^b f_X(x) dx$$

The probability density function of a continuous random variable is non-negative and integrates to 1:
$$\int_{-\infty}^{\infty} f_X(x) dx = 1$$
However, keep in mind that the probability density function can take values greater than 1, since its argument is not a probability but a probability density. The total densities of a **set** of elementary events make up a probability. 


Consider that
$$P_X(X=x) = P_X(x < X < x) = \int_{x}^{x} f_X(u) du = 0$$
This means that
- There are infinitely many possible events in $(a,b)$
- Therefore, the probability that an event takes place at exactly $(a,a)$ is $0$.

---

The cumulative distribution function of two jointly distributed random variables $X$ and $Y$ is defined as:
$$F_{X,Y}(x,y) := P(X \leq x, Y \leq y) = \int_{-\infty}^x \int_{-\infty}^y f_{X,Y}(u,v) dv du$$
where $f_{X,Y}(x,y)$ is the joint probability density function of $X$ and $Y$. The probability density function of $X$ can be obtained by taking the partial derivative of $F_{X,Y}(x,y)$ with respect to $x$:$$f_{X,Y}(x,y) := \frac{\partial}{\partial x} F_{X,Y}(x,y) \quad \forall x,y \in \mathbb{R}$$

The expectation of a continuous random variable is defined as:
$$E[X] := \int_{-\infty}^{\infty} x f_X(x) dx$$
  
The definition of its variance and standard deviation are the same as in the discrete case.

$$"Var"[X] := EE[(X-EE[X])^2] = EE[X^2] - EE[X]^2$$

$$sigma = sqrt("Var"(X))$$