We have leaned how binary classification can be done using probability. From that we get that:
$$log(p_i/(1-p_i)) = w^T x_i quad => p_i = 1/(1+e^(-w^T x_i))$$
The function $sigma(u) = 1/(1+e^(-u))$ is known as the sigmoid function, and the inverse of the sigmoid function is
$$sigma^(-1)(p) = p/(1-p) quad "for some" p, "given that" u = w^T x_i$$
This is called logistic regression.


Now to determine the probability of the whole dataset. We use the following notation
$$PP( y_i | x_i , w) = p_i^(y_i)(1-p_i)^(1-y_i) $$
This way:
- $PP( y_i=0 | x_i , w) = 1-p_i$
- $PP( y_i=1 | x_i , w) = p_i$
![[Pasted image 20260120232310.png]]
(This is an example of how logistic regression uses probability to determine whether mice are obese or not)

And so to determine the whole dataset we can

$$\prod_{i=1}^m p_i^{y_i} (1-p_i)^{1-y_i}$$
Note that products are messy to differentiate, which is nice, for optimization, therefore we can express this as a sum instead:
$$\ell(w) = \log\left(\prod_{i=1}^m p_i^{y_i} (1-p_i)^{1-y_i}\right) = \sum_{i=1}^m\left[y_i \cdot log(p_i) + (1-y_i)\cdot log(1-p_i)\right]$$
Now to express this as a loss function we take the negative log-likelihood, and substitute $p_i=sigma(w^T x_i)$ (the sigmoid)

$$\mathcal{L}_{01}(w) = -\sum_{i=1}^m \log \sigma(w^\top x_i)^{y_i} (1-\sigma(w^\top x_i))^{1-y_i}$$
Now we can differentiate the loss function w.r.t $w$ to find an optimal $w$. That is stupid, so I won't do it here.
