Assume that we now have more than 2 classes (so not binary anymore). Assume that we have $C$ classes, so that
$$S={(x_1,y_1),(x_2,y_2),dots,(x_n,y_n)}, quad "and" quad y in{0,1,C-1} $$
We will need to model the class probabilities of $C$ different classes: $p_1, p_2, \ldots, p_C$:

In binary classification, we discriminated classes with a *sign function*. In multi-class classification we ought to assign a **score for each class**:
$$s_c = w_c^T x_i, quad c=1,2,dots,C$$
Where $s_c in (-oo,oo)$

We want to map these scores to probability. When mapping to probability, we want to make sure that the following constraints hold:
- $0<=p_i<=1$, each probability is constraint as a number between 1 and 1
- $sum_(c=1)^C p_c =1$, the sum of all $p_c$ is always 1.

**Softmax** deals with exactly this, by generalizing the binary classification to $C>2$
$$\sigma(x)_c = \dfrac{e^{x_i}}{\sum_{j=1}^k e^{x_j}}$$
The related loss function is then:$$\mathcal{L}_{CE}(W) = -\sum_{i=1}^m \log \sigma(w_{y_i}^\top x_i) = \sum_{i=1}^m \Big \{ -w_{y_i}^\top x_i + \log \Big ( \sum_{c=1}^C e^{w_c^T x_i} \Big ) \Big \}$$
where $W = [w_1 \ldots w_C]$ is a matrix of the weight vectors for each class. This loss function is called the **cross entropy loss**. We will revisit this loss and understand better why it is given this particular name.

---
Let's try and make an as general algorithm as possible using what we have leaned form regularization
$$L(W) := L_{CE}(W) + \lambda \sum_{c=1}^C ||w_c||_p$$
Like before we can use gradient descent to minimize loss
![[Pasted image 20260120122015.png]]
(example from lecture)