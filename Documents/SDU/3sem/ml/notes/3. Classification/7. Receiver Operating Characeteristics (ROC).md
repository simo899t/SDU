**ROC curve** is a tool to **evaluate the performance of a binary classifier**.
![[Pasted image 20260120232350.png]]
Take this example, to find out where the threshold should be (for the model to be optimal), we can use ROC 



To understand ROC, we need **two rates**:
1. True Positive Rate (TPR)/ sensitivity / Recall
$$"TPR" = "TP"/"TP+FP" $$
2. False Positive Rate (FPR)
$$"FPR"="FP"/"TN + FN"$$
W

The ROC plots these as follows for each:
![[Pasted image 20260120232105.png]]
The more to better thresholds are the ones most at the top left. Now we can choose one, depending on what the model is trying to achieve.

Also, we can use the **AUC** (area under curve) as measurement for how good the classifier is. The higher the AUC, the better the classifier. A perfect classifier has an AUC of 1. A classifier that performs no better than random guessing has an AUC of 0.5.