kNN is based around memorizing the entire dataset and using distance to neighboring data points to classify new data.

This way a new point is classified based on the classes of its **k nearest neighbors** in the training set.

There are multiple ways to do kNN, here I will show 2 alike, yet different ways.
#### Standard kNN
$$\hat{y} = \arg \max_{c \in [C]} \sum_{i=1}^k \mathbb{1}(y_i = c)$$
The standard kNN selects a class to a new data point, only based on the majority vote of the k-nearest neighbors.

#### Weighted kNN
$$\hat{y} = \arg \max_{c \in [C]} \sum_{i=1}^k \mathbb{1}(y_i = c) \dfrac{1}{d(x,x_i)}$$
The weighted kNN does the same, however it normalizes each vote, using the ratio of the distance. This way, points with a small distance to the new point $x$ contribute more to the vote, than points with greater distance to the new point.  

#### Voronoi cells
A **voronoi cell** is the area around a datapoint to which it is the closest point
![[Pasted image 20260120134432.png]]
Example, any new data point within the blue area, will be classified to the blue

When you do this for all data points, you can create a Voronoi map
![[Pasted image 20260120135459.png]]
![[Pasted image 20260120135503.png]]

