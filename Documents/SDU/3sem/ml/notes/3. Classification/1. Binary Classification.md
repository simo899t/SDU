Assume that we have a binary classification problem where
$$S={(x_1,y_1),(x_2,y_2),dots,(x_n,y_n)}, quad "and" quad y in{0,1} $$

Like before we assume the hypothesis to be linear:
$$\mathcal{H} := \{h: h(x) = w^\top x, w \in \mathbb{R}^k \}$$
But in this case the output is now discrete and not continuous.

Let's define one possible way to interpret the label $y$.
- Sign of $h_i$
	- If $h_i>0 -> "class 1"$
	- If $h_i<0 -> "class 0"$
- magnitude of $|h_i|$
	- Larger $|h_i| ->$ higher **confidence**
	- Smaller $|h_i| ->$ lower **confidence**

**Note:** this is just *one* possible way of interpreting a discrete output.

When interpreted this way, the hypothesis $h_i$ is called the **discriminant function**. Such a model is not easy to train since we normally desire a differentiable loss function, which we can't with this "sign" function. By considering this a probability problem would like to achieve:
$$ p_i = w^T x_i quad "WRONG!"$$
The problem with this is that the range of $w^T x_i in (-oo,oo)$ and $p_i in [0,1]$

By taking the log we can widen this so.
$$log(p_i) prop w^T x_i $$
This works well for any $w^T x_i<0$, but is undefined for $w^T x_i >0$ since $log(a) in (-oo,0)$ 
To fix this, we account for the probability of the other class. We actually want to express
$$w^T x_i=log ((PP(y_i = 1|x_i))/(PP(y_i = 0|x_i))) = log(p_i/(1-p_i))$$
This is called logistic function, and we can solve for $p_i$ to find probabilities for both classes.

$$log(p_i/(1-p_i)) = w^T x_i quad => p_i = 1/(1+e^(-w^T x_i))$$
