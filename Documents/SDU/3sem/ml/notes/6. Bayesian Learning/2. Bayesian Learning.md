 The MLE approach cannot model the potential dependency of the variance on the input $x$. That means that beacuse $sigma^2$ is a single number. The problem with having a **single variance** ($\sigma^2$) is that it **assumes all inputs have the same uncertainty**, which is often unrealistic. Let me explain carefully.

The only source of uncertainty is on the precise values of the model parameters, which can be mitigated by collecting more data due to the law of large numbers. This ansatz is called **frequentist learning**. 

In real-world problems, the modeler is never certain about the model and in many cases they are interested in accounting for this source of uncertainty. 

**Bayesian learning** offers a way to do this. Let us see how it works on the same linear regression problem. 
$$w \sim \mathcal{N}(w|0, \alpha^{-1}I)$$
$$y_i|x_i, w \sim \mathcal{N}(y_i|w^T \phi(x_i), \sigma^2), \quad i=1,\ldots,m.$$
This time we are not assuming that $w$ (the distribution) is a fixed parameter, but a random variable. We start with the prior belief that $w \sim p(w)$. Based on the data, we update our belief about $w$ to $p(w|S)$. 

- $p(w)$ is called the **prior distribution** 
- and $p(w|S)$ is called the **posterior distribution**. 

The goal of **learning** in Bayesian machine learning is to calculate the posterior distribution $p(w|S)$ given the dataset $S$. This means that we start with a "unbiased" distribution and update it as we gather more information

This is done via the Bayes' rule:

$$p(w|S) = \frac{p(S|w)p(w)}{p(S)} = \frac{p(S|w)p(w)}{\int p(S|w)p(w) dw}.$$
The term in the denominator $p(S) = \int p(S|w)p(w) dw$ is called the **evidence** (also called the **marginal likelihood**). This means "Given our model (all possible parameter settings), how likely is it that we would see the data we actually observed?"

- In almost every real-world use case, the evidence is hard to calculate because of complexity. Therefore, we will not be able to calculate the posterior distribution exactly. Instead, we will use approximations. The whole field of Bayesian machine learning is about finding good approximations to the posterior distribution.

- The evidence quantifies the fit of the whole model family to data, as it can be viewed as the average likelihood with respect to the prior distribution. Therefore, it can be used for model selection. For instance, if we have two competing models $p(S,w)$ and $p'(S,w')$, we can choose the one with the higher evidence.

Let us next calculate the posterior distribution for the linear regression problem. We have:
  $$\begin{align*}

p(w|S) &= \prod_{i=1}^m \mathcal{N}(y_i|w^T \phi(x_i), \sigma^2) \mathcal{N}(w|0, \alpha^{-1}I) \\

& = \mathcal{N}(w|0, \alpha^{-1}I) \prod_{i=1}^m \mathcal{N}(y_i|w^T \phi(x_i), \sigma^2) \\

& \propto \exp\left(-\frac{1}{2}w^T \alpha I w\right) \prod_{i=1}^m \exp\left(-\frac{1}{2\sigma^2}(y_i-w^T \phi(x_i))^2\right) \\

& \propto \exp\left(-\frac{1}{2}w^T \alpha I w - \frac{1}{2\sigma^2}\sum_{i=1}^m (y_i-w^T \phi(x_i))^2\right) \\

& \propto \exp\left(-\frac{1}{2}w^T \left(\alpha I + \frac{1}{\sigma^2}\sum_{i=1}^m \phi(x_i) \phi(x_i)^T\right) w + \frac{1}{\sigma^2}\sum_{i=1}^m y_i \phi(x_i)^T w\right) \\

& = \mathcal{N}\left(w\left|\left(\alpha I + \frac{1}{\sigma^2}\sum_{i=1}^m \phi(x_i) \phi(x_i)^T\right)^{-1}\frac{1}{\sigma^2}\sum_{i=1}^m y_i \phi(x_i), \left(\alpha I + \frac{1}{\sigma^2}\sum_{i=1}^m \phi(x_i) \phi(x_i)^T\right)^{-1}\right.\right).

\end{align*}$$

A neater notation would be possible considering that the posterior mean calculation involves the inverse of the posterior covariance.
$$p(w | S ) = \mathcal{N}(w|\mu_{post}, \Sigma_{post}),$$
where
$$\begin{align*}

\Sigma_{post} &= \left(\alpha I + \frac{1}{\sigma^2}\sum_{i=1}^m \phi(x_i) \phi(x_i)^T\right)^{-1}\\

\mu_{post} &= \Sigma_{post}\left ( \frac{1}{\sigma^2}\sum_{i=1}^m y_i \phi(x_i) \right ).

\end{align*}$$

  

For a new test input $x_*$, the Bayesian model predicts the output in the form of a distribution:
$$\begin{align*}

p(y_*|S, x_*) &= \int p(y_*|x_*, w) p(w|S) dw \\

&= \int \mathcal{N}(y_*|w^T \phi(x_*), \sigma^2) \mathcal{N}(w|\mu_{post}, \Sigma_{post}) dw \\

&= \mathcal{N}(y_*|\mu_{post}^T \phi(x_*), \sigma^2 + \phi(x_*)^T \Sigma_{post} \phi(x_*)).

\end{align*}
$$
The resulting distribution $p(y_* | S, x_*)$ is called the **posterior predictive distribution**. This distribution has some remarkable properties:

- It considers all possible values of $w$ and averages over them proportionally to their posterior probability. This essential and unique property of Bayesian models is called **model averaging**. It is a way of accounting for model uncertainty.

- Its variance depends on the input $x_*$. Hence, it is able to take into account potential changes in the model confidence in different regions of the input space. This property of a probabilistic model is called **heteroscedasticity**. The Bayesian approach provides this property for free, whereas the frequentist approach requires a separate model for the variance.

In the cases where the predictor is a distribution, there are multiple ways one can use it to make a final prediction. For instance, one can choose the mode of the predictive distribution:
$$\widehat{y} = \arg\max_y p(y_* = y|S, x_*).$$
This is called the **Bayes predictor**. One can alternatively choose to take a sample from the predictive distribution:

$$\widehat{y} \sim p(y_*|S, x_*).$$
This is called the **Gibbs predictor**. Bayes predictor is known to be the optimal predictor in terms of the expected loss.
![[Pasted image 20260121124740.png]]
(Example from lectures of predicted)
