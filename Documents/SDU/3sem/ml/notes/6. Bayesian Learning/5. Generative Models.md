For a supervised learning problem where the data comes from $x,y \sim D$ for an unknown data distribution $D$. We can approach the modeling problem in two ways based on what we want to approximate about the data distribution. Consider that the joint distribution factorizes in two ways:$$p(x,y) = p(y|x)p(x)$$$$p(x,y) = p(x|y)p(y)$$
We can choose the first factorization and account for $p(x)$ using Monte Carlo integration based on the training samples. Then it suffices to approximate $p(y|x)$. This approach is called **discriminative modeling**. The probabilistic and Bayesian linear regressors we developed above were all discriminative models since they fitted $p(y|w,x)$ to data.

An alternative approach would be to choose the second factorization: $p(x,y) = p(x|y)p(y)$ and aim to approximate both $p(y)$ and $p(x|y)$. Notice that this approach attempts to infer the whole data generating process, where a label is first created and the related input is generated. A real-world example could be a person choosing the digit to draw first and then drawing it. The picture of the resulting drawing because the input of the aimed classifier. This approach is called **generative modeling**.

For instance, we can fit a generative model on a data set $S = \{(x_1, y_1), (x_2, y_2), \ldots, (x_m, y_m)\}$ by choosing a parametric distribution family $p(x|y,\theta)$. For a classification problem, we can approximate $p(y)$ by class frequencies, i.e.

$P(y=c) = \frac{1}{m}\sum_{i=1}^m \mathbb{1}(y_i = c)$.

and choose the class-conditional distribution to be:

$$p(x|y=c, \theta_c) = p(x|\theta_{c}),$$

that is, each class shares the same distribution family but has its own parameters. Then we can fit the parameters $\theta_c$ on the training samples belonging to class $c$ by maximum likelihood estimation:

$$\theta_c^{MLE} = \arg\max_{\theta_c} \sum_{ \{i : y_i = c\} } \log p(x_i|\theta_c).$$
Given a test input $x_*$, the generative model predicts the output in the form of a distribution:
$$p(y=c|x_*) = \frac{p(x_*|y=c, \theta_c)P(y=c)}{\sum_{c'} p(x_*|y=c', \theta_{c'})P(y=c')}.$$