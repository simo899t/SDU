MLE assumes that there is even more uncertainty and randomness than previorsly covered. MLE is like rewinding the data-generating process and asking which parameters could have most plausibly produced what we observed. (So we can replicate them)

Let $$S=\{x_1, x_2, \ldots, x_m\}$$
At the stage of modeling, we draw a hypothesis about how this data could have been generated. Let our hypothesis be that the data is generated as independent samples a distribution following a parametric density function $p(x|\theta)$, where $\theta$ is the parameter of the distribution. Then the probability density function of the random variable $S$ representing the occurrence of the dataset is given by
$$p(S|\theta) = p(x_1, x_2, \ldots, x_m | \theta) = \prod_{i=1}^m p(x_i|\theta).$$
If we find $\theta$ we can predict new data in the same shape as $S$

The expression above is a density function and is called the **likelihood** of parameters $\theta$ given the dataset $S$.

We can do this by maximizing the likelihood function $p(S|\theta)$.

This uses "log-likelihood". Where because for $p>0$, $$log(product(P) = sum(log(p)))$$This way we can inspect each $p$. So

$$\theta_{MLE} = \arg\max_\theta \sum_{i=1}^m \log p(x_i|\theta).$$

#### Example
Let us give an example. Choose $p(x|\theta) = \mathcal{N}(x|\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$, where $\theta = (\mu, \sigma^2)$. 

Then consider that a single term of the log-likelihood function for a data point $x_i$ is given by
$$\log p(x_i|\theta) = -\frac{1}{2}\log(2\pi\sigma^2) - \frac{(x_i-\mu)^2}{2\sigma^2}.$$

We can then maximize the likelihood with respect to $\mu$ and $\sigma^2$ by setting the gradient of the full likelihood function to zero and solving for $\mu$.
$$\frac{\partial}{\partial \mu} \sum_{i=1}^m \log p(x_i|\theta) = \sum_{i=1}^m \frac{x_i-\mu}{\sigma^2} = 0 \implies \mu_{MLE} = \frac{1}{m}\sum_{i=1}^m x_i.$$
We can also solve for $\sigma^2$.
$$\frac{\partial}{\partial \sigma^2} \sum_{i=1}^m \log p(x_i|\theta) = -\frac{m}{2\sigma^2} + \frac{1}{2\sigma^4}\sum_{i=1}^m (x_i-\mu)^2 = 0 \implies \sigma^2_{MLE} = \frac{1}{m}\sum_{i=1}^m (x_i-\mu)^2.$$
Note that the parameters for a normal distribution is just the mean $mu$ and variance $sigma^2$

Now that we have found the parameters that fits the data $mu_"MLE"$ and $sigma^2_"MLE"$

Given a test input $x_*$, the learned model predicts the output in the form of a distribution, which is called the **predictive distribution**:

$$y_*|x \sim \mathcal{N}(y_*|w_{MLE}^T \phi(x_*), \sigma^2_{MLE})$$
This determines how likely an output is given an input. If the likelihood is good, when we can use
$$\hat{y} = w^T_{\text{MLE}}\phi(x_*)$$
*If* we have a safety-critical prediction task, we can also build confidence sets. For instance,
$$P\left(|y_* - w_{MLE}^T \phi(x_*)| \leq \sigma_{MLE}\right) = P\left(w_{MLE}^T \phi(x_*) - \sigma_{MLE} \leq y_* \leq w_{MLE}^T \phi(x_*) + \sigma_{MLE}\right) $$
$$= \int_{w_{MLE}^T \phi(x_*) - \sigma_{MLE}}^{w_{MLE}^T \phi(x_*) + \sigma_{MLE}} \mathcal{N}(y_*|w_{MLE}^T \phi(x_*), \sigma^2_{MLE}) dy_* $$
$$= \int_{w_{MLE}^T \phi(x_*) - \sigma_{MLE}}^{w_{MLE}^T \phi(x_*) + \sigma_{MLE}} \frac{1}{\sqrt{2\pi\sigma^2_{MLE}}}\exp\left(-\frac{(y_*-w_{MLE}^T \phi(x_*))^2}{2\sigma^2_{MLE}}\right) dy_*$$
$$= \int_{-1}^{1} \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{z^2}{2}\right) dz = 0.68.$$
