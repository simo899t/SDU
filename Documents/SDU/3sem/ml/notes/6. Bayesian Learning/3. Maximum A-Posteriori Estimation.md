Much like in Bayesian Linear Regression, with **MAP** we can estimate the **posterior mean** of the weight distribution:

$$
w_{\text{MAP}} = \arg\max_w p(w \mid S) = \arg\max_w p(S \mid w) \, p(w) = \arg\max_w \log p(S \mid w) + \log p(w)
$$

The intractable denominator $p(S)$ drops out since it does not depend on $w$. This makes the training objective tractable.

In Bayesian linear regression, the posterior is Gaussian:

$$
p(w \mid S) = \mathcal{N}\big(w \mid \mu_w, \Sigma_w\big)
$$

where the posterior covariance $Sigma_w$ is fully determined by the prior and the data. Once $Sigma_w$ is known, the MAP estimate is simply the **posterior mean**:

$$
w_{\text{MAP}} = \mu_w = \Sigma_w \left( \frac{1}{\sigma^2} \sum_{i=1}^m y_i \phi(x_i) \right)
$$

For a given test input $x^*$, the MAP estimate predicts the output as:

$$
y_* \mid x_* \sim p(y_* \mid w_{\text{MAP}}, x_*)
$$

Since the MAP estimate is a **point estimate**, it does not capture the uncertainty in the model parameters. Once you have $\Sigma_w$​, computing $\text{MAP}$​ is just **matrix multiplication.** 

Because of this, it is not considered a fully Bayesian approach, as it cannot perform model averaging over the posterior.

To do model averaging, you would need the average predictions over the posterior including all its uncertainties.

MAP **ignores this uncertainty**, so it can’t capture:
- Variance in predictions due to uncertain parameters
- Risk of overconfident predictions if data is scarce
- True Bayesian model averaging benefits