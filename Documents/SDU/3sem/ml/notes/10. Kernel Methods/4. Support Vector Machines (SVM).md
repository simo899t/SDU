Among many possible hyperplanes that may separate the data points of two classes, SVM chooses the one that maximizes the smallest distance between a data point and the hyperplane. 

This distance is called the **margin**. The data points that are closest to the hyperplane are called **support vectors**.

Note that we use because of these property, SVM is also called a **maximum margin classifier**. We would normally use k-fold cross validation, and balance the bias-variance tradeoff to ensure a maximum margin.

![[Pasted image 20260120175429.png]]
In this example is 2d, where the support vectors are lines