{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOGvxi758TPB"
   },
   "source": [
    "# AI512: Introduction to Machine Learning\n",
    "## University of Southern Denmark - IMADA\n",
    "### Fall 2024 - Melih Kandemir\n",
    "\n",
    "---\n",
    "# Exercise 06\n",
    "---\n",
    "\n",
    "- Maximum Likelikhood Estimation and Bayes Theorem\n",
    "- Probability Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4gu9iNd8TPD"
   },
   "source": [
    "### Maximum Likelikhood Estimation and Bayes Theorem\n",
    "\n",
    "Assume the following random samples following a Binomial distribution:\n",
    "\n",
    "$X_i \\sim \\text{Binomial}(3, \\theta)$, which represents the number of successes in a fixed number of independent trials (in this example: 3), where some former observations are provided as follows: $S = \\{x_1, x_2, x_3, x_4\\} = \\{1, 3, 2, 2\\}$.\n",
    "\n",
    "- Find the likelihood function for this distribution on the data set $S$.\n",
    "- Identify the model parameters.\n",
    "- Find the maximum likelihood estimate of the parameter θ.\n",
    "- Make the model Bayesian by introducing a prior distribution on the Binomial distribution parameter $\\theta$. Let the prior be a Beta distribution with parameters α=1 and β=2.\n",
    "- Calculate the posterior distribution.\n",
    "    - Normalize the posterior distribution with the normalization constant.\n",
    "    - Hint: If the PDF of the Beta distribution is given as $p(\\theta|\\alpha, \\beta) = \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1} / Z $, \n",
    "    \n",
    "      then:\n",
    "      $p(\\theta|A+\\alpha, B+\\beta)/Z'$ is still a Beta distribution.\n",
    "    - The normalization constant for the Beta distribution is given by $C = \\text{Beta}(a, b) = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}$, where for positive integers, $\\Gamma(x) = (x-1)!$.\n",
    "    - Use the `gamma` function from the `scipy.special` library to calculate the normalization constant.\n",
    "\n",
    "- Predict 20 new predictions for the random variable $X$ using 3 trials and parameter $\\theta$ sampled from posterior disribution.\n",
    "    -  Calculate their mean.\n",
    "    -  Calculate their standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvFIOV578TPD"
   },
   "source": [
    "#### Solution of Maximum Likelikhood Estimation and Bayes Theorem\n",
    "\n",
    "First let us remember the definition of the Binomial distribution:\n",
    "\n",
    "$\\text{Binomial}(n, p) = \\binom{n}{k}p^k(1−p)^{n−k}$ where n is the number of trials, p is the probability of success, and k is the number of successes. \n",
    "\n",
    "- Find the likelihood function for this distribution on the data set $S$.\n",
    "\n",
    "If $X_i \\sim \\text{Binomial}(3,\\theta)$, then $P_{X_i}(x | \\theta) = \\binom{3}{x} \\theta^x (1-\\theta)^{3-x}$\n",
    "\n",
    "The definition of the likelihood function is given as follows:\n",
    "\n",
    "$L(S | \\theta) = \\prod_{i=1}^S P_{X_i}(x_i | \\theta)$\n",
    "\n",
    "Then the likelihood function for this distribution on the data set $S$ is given as follows:\n",
    "\n",
    "\\\n",
    "\\begin{align*}\n",
    "L(S | \\theta) &= \\prod_{i=1}^S P_{X_i}(x_i | \\theta) \\\\\n",
    "&= \\prod_{i=1}^S \\binom{3}{x_i} \\theta^{x_i} (1-\\theta)^{3-x_i} \\\\\n",
    "&= \\binom{3}{1} \\theta^{1} (1-\\theta)^{3-1} \\times \\binom{3}{3} \\theta^{3} (1-\\theta)^{3-3} \\times \\binom{3}{2} \\theta^{2} (1-\\theta)^{3-2} \\times \\binom{3}{2} \\theta^{2} (1-\\theta)^{3-2} \\\\\n",
    "&= 3\\theta^{1} (1-\\theta)^{2} \\times 1 \\times \\theta^{3} \\times 3 \\theta^{2} (1-\\theta)^{1} \\times 3 \\theta^{2} (1-\\theta)^{1} \\\\\n",
    "&= 3 \\theta^{1} (1-\\theta)^{2} \\times \\theta^{3} \\times 3 \\theta^{2} (1-\\theta)^{1} \\times 3 \\theta^{2} (1-\\theta)^{1} \\\\\n",
    "&= 3^3 \\theta^{1+3+2+2} (1-\\theta)^{2+0+1+1} \\\\\n",
    "&= 27 \\theta^{8} (1-\\theta)^{4} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "- Identify the model parameters.\n",
    "\n",
    "$\\theta$ is the parameter of the Binomial distribution, representing the probability of success for each trial. It is the parameter that we want to estimate or infer based on the observed data.\n",
    "\n",
    "- Find the maximum likelihood estimate of the parameter $\\theta$.\n",
    "\n",
    "The maximum likelihood estimate of the parameter $\\theta$ is the value of $\\theta$ that maximizes the likelihood function $L(S | \\theta)$. In oder to find the maximum likelihood estimate of the parameter $\\theta$, we need to find the value of $\\theta$ that sets the derivative of the likelihood function $L(S | \\theta)$ to zero.\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{d}{d\\theta} L(S | \\theta) &= \\frac{d}{d\\theta} \\left( 27 \\theta^{8} (1-\\theta)^{4} \\right) \\\\\n",
    "0 &= \\frac{d}{d\\theta} \\left( 27 \\theta^{8} (1-\\theta)^{4} \\right) \\\\\n",
    "0 &= \\frac{d}{d\\theta} \\left( \\theta^{8} (1-\\theta)^{4} \\right) \\\\\n",
    "0 &= 8\\theta^{7}(1-\\theta)^{4} - 4\\theta^{8}(1-\\theta)^{3} \\\\\n",
    "0 &= 8(1 - \\theta) - 4\\theta \\\\\n",
    "0 &= 8 - 12\\theta \\\\\n",
    "12\\theta &= 8 \\\\\n",
    "\\theta &= \\frac{8}{12} \\\\\n",
    "\\theta &= \\frac{2}{3} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "- Make the model Bayesian by introducing a prior distribution on the Binomial distribution parameter $\\theta$. Let the prior be a Beta distribution with parameters $\\alpha=1$ and $\\beta=2$.\n",
    "\n",
    "To make the model Bayesian, we need to assign a prior distribution to the parameter $\\theta$ and then use Bayes' theorem to update the posterior distribution of $\\theta$ based on the observed data.\n",
    "\n",
    "The prior distribution represents our beliefs or uncertainty about the parameter $\\theta$ before observing any data. We assume the prior as a Beta distribution with parameters $\\alpha=1$ and $\\beta=2$.\n",
    "\n",
    "\n",
    "\n",
    "The Beta distribution is given as follows:\n",
    "\n",
    "$\\text{Beta}(\\theta | \\alpha, \\beta) = \\theta^{\\alpha-1} (1-\\theta)^{\\beta-1} / Z $\n",
    "\n",
    "where $Z$ is the normalization constant.\n",
    "\n",
    "Then the posterior distribution is proportional to the product of the likelihood function and the prior distribution:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{posterior} = p(\\theta | S) &\\propto L(S | \\theta) \\times p(\\theta) \\\\ \n",
    "&\\propto L(S | \\theta) \\times \\text{Beta}(\\theta | \\alpha, \\beta) \\\\ \n",
    "&\\propto 27 \\theta^{8} (1-\\theta)^{4} \\times \\theta^{\\alpha - 1} (1-\\theta)^{\\beta - 1} \\\\\n",
    "&\\propto 27 \\theta^{8} (1-\\theta)^{4} \\times \\theta^{1 - 1} (1-\\theta)^{2 - 1} \\\\\n",
    "&\\propto 27 \\theta^{8} (1-\\theta)^{4} \\times \\theta^{0} (1-\\theta)^{1} \\\\\n",
    "&\\propto 27 \\theta^{8} (1-\\theta)^{4} \\times (1-\\theta)^{1} \\\\\n",
    "&\\propto 27 \\theta^{8} (1-\\theta)^{5} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "- Calculate the posterior distribution.\n",
    "\n",
    "To make the posterior distribution a proper probability distribution, we need to normalize it by dividing it by the normalization constant $Z'$.\n",
    "\n",
    "Posterior distribution can be considered as an unnormalized Beta distribution with parameters $\\alpha' = 8 + 1 = 9$ and $\\beta' = 5 + 1 = 6$. Then the normalization constant $Z'$ is given as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "Z' &= \\text{Beta}(\\alpha', \\beta') \\\\\n",
    "   &= \\frac{\\Gamma(\\alpha')\\Gamma(\\beta')}{\\Gamma(\\alpha' + \\beta')} \\\\\n",
    "   &= \\frac{\\Gamma(9)\\Gamma(6)}{\\Gamma(9 + 6)} \\\\\n",
    "   &= \\frac{8! \\times 5!}{14!} \\\\\n",
    "   &= \\frac{40320 \\times 120}{87178291200} \\\\\n",
    "   &= \\frac{4838400}{87178291200} \\\\\n",
    "   &= \\frac{1}{18018} \\\\\n",
    "   &\\approx 0.0000555 \\\\\n",
    "\\end{align*}\n",
    "\n",
    "We can also calculate the normalization constant $Z'$ using the `gamma` function from the `scipy.special` library:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Mc0gaL_ZwaeL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.55000555000555e-05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import binom\n",
    "import scipy.special\n",
    "\n",
    "# Beta function parameters\n",
    "alpha = 9\n",
    "beta = 6\n",
    "\n",
    "# Calculate the normalization constant using the beta function formula\n",
    "normalization_constant = (scipy.special.gamma(alpha) * scipy.special.gamma(beta)) / scipy.special.gamma(alpha + beta)\n",
    "\n",
    "print(normalization_constant)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the posterior distribution can be written as Beta distribution with parameters $\\alpha' = 9$ and $\\beta' = 6$ as follows: $\\theta \\sim \\text{Beta}(9, 6)$.\n",
    "\n",
    "\n",
    "- Predict 20 new predictions for the random variable $X$ using 3 trials and parameter $\\theta$ sampled from posterior disribution.\n",
    "\n",
    "To predict a new value for the random variable $X$, we need to sample $θ$ from posterior distribution and generate values from the Binomial distribution.\n",
    "\n",
    "$X_{predicted} \\sim \\text{Binomial}(3, \\theta)$ where $\\theta \\sim \\text{Beta}(9, 6)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New predictions from Binomial with sampled theta: [2, 3, 2, 3, 3, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 3, 2, 1, 3, 1]\n",
      "mean of predictions: 2.0\n",
      "standard deviation of predictions: 0.7071067811865476\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import beta\n",
    "\n",
    "# Parameters of the Beta posterior distribution\n",
    "alpha = 9\n",
    "beta = 6\n",
    "\n",
    "# Number of predictions\n",
    "num_predictions = 20\n",
    "\n",
    "# Initialize an array to store the predictions\n",
    "predictions = []\n",
    "\n",
    "# Perform 20 predictions\n",
    "for _ in range(num_predictions):\n",
    "    # Sample theta from the Beta posterior distribution\n",
    "    beta_dist =  stats.beta(alpha, beta)\n",
    "    sampled_theta = beta_dist.rvs()\n",
    "    \n",
    "    # Generate a new value based on the sampled theta\n",
    "    new_value = np.random.binomial(3, sampled_theta)  # Assuming 3 trials in the Binomial experiment\n",
    "    \n",
    "    # Append the new value to the predictions list\n",
    "    predictions.append(new_value)\n",
    "\n",
    "print(\"New predictions from Binomial with sampled theta:\",predictions)\n",
    "print(\"mean of predictions:\",np.mean(predictions))\n",
    "print(\"standard deviation of predictions:\",np.std(predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chP5dZq38TPE"
   },
   "source": [
    "### Probabilty Theory\n",
    "A coin is weighted so that its probability of landing on heads is 20%, independently of other flips. Suppose the coin is flipped 20 times. \n",
    "- Calculate the actual probability that this coin lands on heads at least 16 times out of 20 flips.\n",
    "- Use Markov’s inequality to bound this probability.\n",
    "- Bound this probability using Chebyshev’s inequality.\n",
    "- Which of the above inequalities provides a tighter bound on the true probability value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution of Probabilty Theory\n",
    "\n",
    "- Calculate the actual probability that this coin lands on heads at least 16 times out of 20 flips.\n",
    "\n",
    "Let $X$ be the random variable representing the number of heads in 20 flips of the coin. Then $X \\sim \\text{Binomial}(20, 0.2)$.\n",
    "\n",
    "\\begin{align*}\n",
    "P(X \\geq 16) &= \\sum_{x=16}^{20} P(X = x) \\\\\n",
    "             &= \\sum_{x=16}^{20} \\binom{20}{x} 0.2^x (1-0.2)^{20-x} \\\\\n",
    "             &= \\binom{20}{16} 0.2^{16} (0.8)^{4} + \\binom{20}{17} 0.2^{17} (0.8)^{3} + \\binom{20}{18} 0.2^{18} (0.8)^{2} + \\binom{20}{19} 0.2^{19} (0.8)^{1} + \\binom{20}{20} 0.2^{20} (0.8)^{0} \\\\\n",
    "             &\\approx 1.38 \\cdot 10^{-8}\n",
    "\\end{align*}\n",
    "\n",
    "- Use Markov’s inequality to bound this probability.\n",
    "\n",
    "\n",
    "For a non-negative random variable $X$ and a positive real number $k$, Markov’s inequality is given as follows:\n",
    "\n",
    "$P(X \\geq k) \\leq \\frac{E[X]}{k}$.\n",
    "\n",
    "Equivalently, we can write Markov’s inequality as follows (plugging in $kE[X]$ for $k$):\n",
    "\n",
    "$P(X \\geq k E[X]) \\leq \\frac{1}{k}$.\n",
    "\n",
    "In our case, $X$ is a non-negative random variable and $k$ is a positive real number. Then Markov’s inequality is given as follows:\n",
    "\n",
    "$P(X \\geq 16) \\leq \\frac{E[X]}{16}$.\n",
    "\n",
    "We know that the expected value of a Binomial distribution is given as follows:\n",
    "\n",
    "$E[X] = np$\n",
    "\n",
    "where $n$ is the number of trials and $p$ is the probability of success for each trial.\n",
    "\n",
    "In our case, $n = 20$ and $p = 0.2$. Then the expected value of $X$ is given as follows:\n",
    "\n",
    "$E[X] = np = 20 \\times 0.2 = 4$.\n",
    "\n",
    "Then Markov’s inequality is given as follows:\n",
    "\n",
    "$P(X \\geq 16) \\leq \\frac{E[X]}{16} = \\frac{4}{16} = 0.25$.\n",
    "\n",
    "\n",
    "- Bound this probability using Chebyshev’s inequality.\n",
    "\n",
    "For a random variable $X$ with mean $\\mu$, variance $\\sigma^2$ and any real number $k > 0$, Chebyshev’s inequality is given as follows:\n",
    "\n",
    "$P(|X - \\mu| \\geq k\\sigma) \\leq \\frac{1}{k^2}$.\n",
    "\n",
    "In our case, $X$ is a random variable with mean $\\mu = 4$ and variance $\\sigma^2 = np(1-p) = 20 \\times 0.2 \\times (1-0.2) = 3.2$. \n",
    "\n",
    "Note that Chebyshev’s inequality asks about the difference in either direction of the RV from its mean, so we need to consider both $P(X \\geq 16)$ and $P(X \\leq -8)$. The reason we chose $-8$ is because $-8$ is Chebyshev’s inequality is symmetric around the mean (difference of 12; $4 \\pm 12$ gives the interval $[-8, 16]$).\n",
    "\n",
    "Then Chebyshev’s inequality is given as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "P(X \\geq 16) &\\leq P(X \\geq 16 \\cup X \\leq -8)  & \\text{Adding another event can only increase probability} \\\\\n",
    "             &= P(|X - 4| \\geq 12) & \\text{Definition of absolute value} \\\\\n",
    "             &= P(|X - E[X]| \\geq 12) & \\text{Since } E[X] = 4 \\\\\n",
    "             &\\leq \\frac{\\text{Var}(X)}{12^2} & \\text{Chebyshev’s inequality} \\\\\n",
    "             &= \\frac{3.2}{12^2} \\\\\n",
    "             &= \\frac{3.2}{144} \\\\\n",
    "             &= \\frac{1}{45} \\\\\n",
    "             &\\approx 0.0222 \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "- Which of the above inequalities provides a tighter bound on the true probability value.\n",
    "\n",
    "Markov’s inequality take only the mean into account while Chebyshev’s inequality takes the mean and variance into account.\n",
    "Therefore Chebyshev is a much better bound than given by Markov’s inequality, but still far from the actual probability. This is because there is so much more information about a random variable can be considered than just these two quantities!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
