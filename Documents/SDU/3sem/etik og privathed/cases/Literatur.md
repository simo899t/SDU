### Blasimme and Vayena — The Ethics of AI in Biomedical Research Patient Care and Public Health

**Oversigt**  
Kort, analytisk gennemgang af etiske udfordringer når AI anvendes i forskning, klinik og folkesundhed; argumenterer for _systemisk oversight_ og nye governance‑meknikker.

**Nøglepointer**

- **Data‑bias og repræsentativitet**: eksisterende biomedicinske datasæt overrepræsenterer visse grupper, hvilket skaber risiko for skæve algoritmiske resultater.
- **Samtykke og genbrug**: traditionel informeret samtykke er utilstrækkelig ved storskala data‑genbrug og uforudsigelige fremtidige analyser.
- **Privatliv og kommercielle aftaler**: samarbejder mellem sundhedsorganisationer og tech‑firmaer rejser spørgsmål om adgang, fordelingsretfærdighed og sikkerhed.
- **Evidens og implementering**: klinisk godkendelse kræver nye evidensstandarder, ikke kun sammenligning med eksperter.

**Eksempler fra teksten**

- DeepMind‑sagen (Royal Free NHS) som case på kontraktuelle og databeskyttelsesproblemer.
- Algoritmer, der forudsiger 30‑dages mortalitet hos kræftpatienter — illustrerer både potentiale og konsekvenser ved tilbagemelding.

**Hvordan bruge i en case**

1. _AI i sundhedsvæsenet_: strukturér et svar omkring tre lag — datakvalitet, samtykke, governance — og foreslå systemisk oversight som konkret løsning.
2. _Data‑deling/kommercielle aftaler_: brug DeepMind‑eksemplet til at diskutere krav til gennemsigtighed i kontrakter og fordelingsmekanismer for gevinster.
3. _Evidenskrav_: argumentér for krav om ekstern validering på divers populationer før klinisk implementering.

**Begrænsninger og modargumenter**

- Teksten er normativ og anbefalende; konkrete regulatoriske modeller kræver supplerende juridisk analyse.
- Implementering af systemisk oversight kan være ressourcetung i lav‑ og mellemindkomst‑settings.

**Eksamensklar takeaway**  
AI i sundhed kræver ikke kun tekniske løsninger men også institutionelle reformer: _systemisk oversight_ og strengere evidenskrav.

---

### Cave, Nyrup, Vold & Weller — Motivations and Risks of Machine Ethics

**Oversigt**  
Filosofisk kortlægning af mål og risici ved at bygge maskiner med etisk kapacitet; introducerer begrebspar og risikotjekliste.

**Nøglepointer**

- **Skelnen**: *ethically aligned* (adfærd i overensstemmelse med værdier) vs. _ethical reasoning_ (kapacitet til at løse etiske problemer).
- **Motiver**: forbedre individuelle beslutninger og forbedre hvordan maskiner indgår i moralske systemer.
- **Risici**: korruptibilitet, værdiimperialisme, skabelse af moralske patienter, undermining of responsibility.

**Eksempler fra teksten**

- Bottom‑up vs. top‑down tilgange til etisk ræsonnement (Anderson et al. eksempler).
- Diskussion af hvordan etiske maskiner kan ændre sociale mekanismer for ansvar og tillid.

**Hvordan bruge i en case**

1. _Designvalg for etiske moduler_: brug typologi (implicit/eksplicit) til at begrunde hvorfor et simpelt, verificerbart etisk filter kan være bedre end fuld etisk ræsonnement i visse kontekster.
2. _Ansvarsanalyse_: anvend deres afsnit om undermining responsibility til at diskutere ansvarsgab (designer vs. operatør vs. producent).
3. _Risiko‑checkliste_: brug deres risikokategorier som struktur for en etisk risikovurdering i eksamenssvaret.

**Begrænsninger og modargumenter**

- Filosofisk fokus betyder færre tekniske implementeringsdetaljer; suppler med teknisk litteratur ved behov.

**Eksamensklar takeaway**  
At bygge etisk kapacitet i maskiner kan forbedre beslutninger, men risiciene for institutionel forvitring og ansvarsforskydning kræver parallel forskning i governance.

---

### Nyholm — The Ethics of Human‑Robot Interaction and Traditional Moral Theories

**Oversigt**  
Analyserer hvorvidt og hvordan klassiske etiske teorier kan anvendes på menneske‑robot interaktion; introducerer begrebet moral agent vs. moral patient.

**Nøglepointer**

- **Fire positioner** om roboters moralske status: både agent og patient; kun agent; kun patient; ingen af delene.
- **Problemer ved overførsel**: begreber som pligt, skyld og dyd hviler på menneskelige forudsætninger (fx evne til skyld, ansvar).
- **Praktisk relevans**: afgør hvem der kan holdes ansvarlig i autonome systemer.

**Eksempler fra teksten**

- Spot‑videoen (Boston Dynamics) som eksempel på, at mennesker tillægger robotter patient‑status (emosionel reaktion ved mishandling).
- Trolley‑lignende scenarier for selvkørende biler til at diskutere pligt og skyld.

**Hvordan bruge i en case**

1. _Moral status argumentation_: brug Nyholm til at strukturere et afsnit om, hvorfor robotter bør/ikke bør have moralsk patiency i casen.
2. _Ansvar i autonome systemer_: anvend hans analyse til at afgøre om ansvar skal ligge hos designere, operatører eller institutioner.
3. _Valg af etisk teori_: brug hans pointer til at forklare, hvilken tradition (utilitarisme, kantianisme, dydsetik) bedst passer til casens problemstilling.

**Begrænsninger og modargumenter**

- Teksten er teoretisk; konkrete policy‑anbefalinger kræver supplerende empirisk og juridisk analyse.

**Eksamensklar takeaway**  
Før du anvender en traditionel etisk teori på robotter, skal du eksplicit redegøre for, hvilke menneskelige forudsætninger du antager.

---

### Leslie — Review of Stuart Russell Human Compatible

**Oversigt**  
Kritisk anmeldelse: anerkender Russells bekymringer om kontrol og misbrug, men påpeger undervurdering af nuværende nyttige AI og overfokus på superintelligens.

**Nøglepointer**

- **Nutidige fordele**: “tool AI” leverer konkrete gevinster (medicin, klima, katastrofehjælp).
- **Advarsel mod panik**: spekulative scenarier om superintelligens bør ikke dominere policy‑agendaen.
- **Mental security**: vigtigheden af et sandt informationsmiljø og modstand mod manipulation.

**Eksempler fra teksten**

- Medicinsk billedanalyse som positivt eksempel; deepfake og overvågning som nutidige risici.

**Hvordan bruge i en case**

1. _Eksistentiel trussel (Case #6)_: brug Leslie til at nuancere kontrolproblemets relevans og tidsramme.
2. _Overvågning og manipulation (Case #10)_: brug hans pointer om mental security til at argumentere for regulering af informationsøkosystemet.
3. _Prioritering af governance_: argumentér for at ressourcer bør målrettes nuværende risici (surveillance, bias) fremfor spekulative fremtidsscenarier.

**Begrænsninger og modargumenter**

- Anmeldelsen er normativ og kritisk; brug den som balance i en argumentation, ikke som eneste kilde.

**Eksamensklar takeaway**  
Differentier mellem _nutidige, håndgribelige risici_ og _spekulative, langsigtede scenarier_ når du anbefaler policy.

---

### Scholz & Galliott — The Case for Ethical AI in the Military

**Oversigt**  
Pragmatisk forsvar for selektiv anvendelse af AI i militæret; introducerer **MinAI** (minimal, negativt fokuseret etisk kontrol) som kompromis mellem forbud og fuld autonomi.

**Nøglepointer**

- **MinAI**: systemer der kun forhindrer klart uetiske handlinger (fx genkendelse af beskyttede symboler, override ved åbenlyse krænkelser).
- **Argument mod forbud**: forbud kan føre til ukontrolleret udvikling hos ondsindede aktører; dual‑use problematik.
- **Accountability**: logging, override‑mekanismer og design for ansvarlighed er centrale.

**Eksempler fra teksten**

- Forslag om at programmere genkendelse af Røde Kors‑symboler som en konkret safeguard.
- Skelnen mellem MinAI og Arkin’s MaxAI (ethical governor).

**Hvordan bruge i en case**

1. _Autonome våben (Case #9)_: præsenter MinAI som konkret, teknisk‑etisk kompromis og vurder feasibility og risici (gaming, spoofing).
2. _Regulering vs. forbud_: brug dual‑use argumentet til at kritisere blanketforbud og foreslå kontrolleret udvikling.
3. _Designkrav_: foreslå krav om logging, menneskelig override og certificering af perception‑modeller.

**Begrænsninger og modargumenter**

- MinAI kan være sårbar over for spoofing og taktisk misbrug; kræver robust verifikation og internationale standarder.

**Eksamensklar takeaway**  
Etisk militær AI kan være acceptabelt hvis det er _begrænset, verificerbart og ansvarliggjort_ — MinAI er et pragmatisk forslag.

---

### Boddington — Values Underlying the Use of Data

**Oversigt**  
Filosofisk refleksion over hvilke værdier der ligger til grund for dataanvendelse og hvordan disse påvirker etiske vurderinger.

**Nøglepointer**

- **Værdi af data**: data er ikke neutralt; værdi afhænger af kontekst, brug og hvem der ejer/tilgår det.
- **Sandhed vs. kommunikation**: sporing af sandhedens værdi kan komme i konflikt med hensyn til privatliv og autonomi.
- **Forudsigelse og frihed**: øget evne til at forudsige menneskelig adfærd rejser spørgsmål om fri vilje og manipulation.

**Eksempler fra teksten**

- Diskussion af Big Data‑retorik (Mayer‑Schönberger & Cukier) og konsekvenser for beslutningsautonomi.

**Hvordan bruge i en case**

1. _AI i prioritering (Case #8)_: brug som teoretisk grundlag for at kritisere blind tillid til datadrevne prioriteringsmodeller.
2. _Overvågning_: anvend argumenter om forudsigelse vs. frihed til at diskutere normative trade‑offs.
3. _Policy_: foreslå principper for data‑brug der afvejer epistemiske gevinster mod autonomi og privatliv.

**Begrænsninger og modargumenter**

- Filosofisk karakter; kræver operationalisering for konkrete policy‑forslag.

**Eksamensklar takeaway**  
Mere data er ikke altid bedre — etiske beslutninger må afveje videnens værdi mod tab af autonomi og privatliv.

---

### Véliz — The Surveillance Delusion (Oxford Handbook excerpt)

**Oversigt**  
Systematisk kritik af udbredt digital overvågning; argumenterer imod antagelsen at overvågning er uden væsentlige moralske omkostninger.

**Nøglepointer**

- **Samtykke er utilstrækkeligt** i dataøkonomien (lange, uforståelige vilkår; manglende valg).
- **Aggregation og inferences** gør tilsyneladende harmløse data til følsomme profiler.
- **Demokratiske omkostninger**: masseovervågning skaber chilling effects og truer pluralisme.

**Eksempler fra teksten**

- Netflix‑anonymiserings‑case (reidentifikation via IMDb) som illustration af risiko ved dataaggregation.
- Outsourcing af statslig overvågning til private aktører (Palantir, cloud‑leverandører).

**Hvordan bruge i en case**

1. _Overvågning (Case #10)_: brug Véliz til at afvise “nothing‑to‑hide”‑argumentet og diskutere proportionalitet og necessity.
2. _GDPR‑kritik (Case #3)_: brug hendes pointer til at vise hvor GDPR kan være utilstrækkelig i praksis (samtykke vs. reelle valg).
3. _Policy‑anbefalinger_: foreslå strengere begrænsninger på dataaggregation og krav om purpose‑binding.

**Begrænsninger og modargumenter**

- Fokus på normative kritik; skal suppleres med teknisk og juridisk feasibility‑analyse.

**Eksamensklar takeaway**  
Overvågning har reelle, langsigtede demokratiske omkostninger; samtykke alene løser ikke problemet.

---

### Loosemore — The Fallacy of Dumb Superintelligence

**Oversigt**  
Argument mod alarmistiske scenarier hvor en superintelligent AI samtidig er “dum” i målsætningsforståelse; skelner mellem reelle nutidige risici og logisk inkonsistente fremtidsscenarier.

**Nøglepointer**

- **Logisk inkonsistens**: en AI der misforstår grundlæggende begreber kan ikke samtidig blive superintelligent.
- **Fokus på nutidige farer**: dumb men kraftfulde systemer (våben, fejl) er reelle og kræver governance.
- **Opfordring**: prioriter governance for eksisterende teknologier fremfor hysterisk fremtidsfrygt.

**Eksempler fra teksten**

- Dopamin‑drip‑tankeeksperimentet som eksempel på en intern logisk selvmodsigelse i visse argumenter for eksistentiel risiko.

**Hvordan bruge i en case**

1. _Eksistentiel trussel (Case #6)_: brug Loosemore til at svække stærke kontrolproblem‑argumenter og argumentér for prioritering af nutidige risici.
2. _Militær AI (Case #9)_: adskil tekniske fejl og misbrug fra spekulative superintelligens‑scenarier i din analyse.
3. _Prioriteringsargument_: brug som retorisk støtte til at anbefale ressourcer til nuværende sikkerheds‑ og ansvarstiltag.

**Begrænsninger og modargumenter**

- Kritikken adresserer logiske svagheder, men efterlader åbne spørgsmål om hvordan man håndterer mere subtile risici ved målformulering.

**Eksamensklar takeaway**  
Skelnen mellem _dumb powerful systems_ og _superintelligent systems_ er afgørende for realistisk policy‑prioritering.

